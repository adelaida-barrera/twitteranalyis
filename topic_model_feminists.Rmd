---
title: "Topic model feminists"
author: "Adelaida Barrera"
date: "12/9/2020"
output: html_document
---

```{r}
library(tidyverse)
library(stm)
library(quanteda)
library(tidytext)
library(ggthemes)
```

# Get data and clean it. Create dfm. 
```{r}

# Load data with tweets from individual (not congresswomen) feminists' twitter accounts between july and november 2020. 

load("data/non_congress_fem_jul_nov_2020.RData")

non_congress <- non_congress_fem_jul_nov_2020

non_congress <- 
  non_congress %>% 
  select(name, 
         screen_name, 
         text,
         user_id, 
         created_at, 
         retweet_count,
         main_occupation,
         institution)

# Remove accounts that do not talk about gender much
non_congress <- filter(name != c("Rordigo Sandoval", "Sara Tufano", "Juan Carlos Rincón Escalante"))

#Creating corpus
non_congress_corp <- corpus(non_congress)

# Removing unnecessary words; stem the text, extract n-grams, remove punctuation, keep Twitter features…
non_congress_dfm <- dfm(non_congress_corp, 
             tolower = TRUE, # convert all words to lower case 
             remove = c(stopwords("spanish")),
             remove_punct = TRUE, # remove punctuation
             remove_url = TRUE, # remove u
             remove_numbers = TRUE,
             remove_symbols = TRUE,
             verbose = TRUE) # Bueno hacer esta opción para ver que está haciendo el dfm.
            #it is better not to stem for topic models  
             
# Remove mentions (@)
non_congress_dfm <- dfm_remove(non_congress_dfm,
                      "@*",
                      verbose = T)

#Remove hashtags         
non_congress_dfm <- dfm_remove(non_congress_dfm,
                      "#*",
                      verbose = T)

# Remove more stopwords (less than 4 characters)
non_congress_dfm <- dfm_keep(non_congress_dfm,
                    min_nchar = 4,
                      verbose = T)

# Remove additional words that do not do meaningful work 
load("additional_stopwords.RData")

non_congress_dfm <- dfm_remove(non_congress_dfm,
                    additional_stopowrds,
                      verbose = T)

#Creating a random sample of 7000 tweets to test the topic model (full dfm is too large)
sample_non_congress <- dfm_sample(x = non_congress_dfm, size = 7000, margin = "documents")
```

#Running the structural topic model

```{r}

#Make the quanteda dfm a stm corpus for running the model
stm_sample_non_congress <- asSTMCorpus(sample_non_congress)

# Getting info from the corpus
out <- prepDocuments(stm_sample_non_congress$documents, 
                     stm_sample_non_congress$vocab, 
                     stm_sample_non_congress$meta)

# Running the structural topic model, defining 10 topics. 
model_topics_non_congress <- stm(documents = out$documents, 
              vocab = out$vocab, 
              K = 10, seed = 12345)

# Save the output for future plotting or analysis 
save(model_topics_non_congress, file = "data/pruebas/model_topics_non_congress")

```

#See what came out 

```{r}

plot(model_topics_non_congress, type = "labels", labeltype = "prob", topics = (1:3)) # or frex, lift, score
plot(model_topics_non_congress, type = "labels", labeltype = "prob", topics = (4:6)) 
plot(model_topics_non_congress, type = "labels", labeltype = "prob", topics = (7:9)) 
plot(model_topics_non_congress, type = "labels", labeltype = "prob", topics = 10)
 
```
# Plotting prevalence of topics in corpus 

```{r}

# Trying to plot as in https://juliasilge.com/blog/evaluating-stm/
# Hay que entender bien qué significan las betas y las gammas :S 

td_beta <- tidy(model_topics_non_congress)

td_gamma <- tidy(model_topics_non_congress, matrix = "gamma",
                 document_names = rownames(non_congress_dfm))

top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

topics_ind_plot <- gamma_terms %>%
  top_n(20, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0.2, nudge_y = 0.0005, size = 2.5) +
  coord_flip() +
  theme_minimal() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.3)) +
  labs(x = NULL, y = expression(gamma),
       title = "Topics by prevalence in the Colombian feminist twitter discourse",
       subtitle = "Top words that contribute to each topic")

ggsave("figures/topics_ind_plot.jpg")
```

```{r}

load("model_topics_non_congress")

model_topics <- tidy(model_topics_non_congress, matrix = "beta")
model_topics

model_top_terms <- model_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#Identify the words that are most common within topics

model_top_terms %>% 
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered() +
  theme_minimal()

```

```{r}

#Per-document classification -Gamma

#Each of these values is an estimated proportion of words from that document that are generated from that topic

documents_gamma <- tidy(model_topics_non_congress, matrix = "gamma")

doc_top1 <- documents_gamma_df %>% 
  filter()

```

```{r}
documents_gamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)
  


```