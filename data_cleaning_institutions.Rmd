---
title: "Institutional data cleaning and exploring"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(quanteda)
library(readtext) # for getting document and their info into a data frame 
library(tm)
library(lubridate)
library(ggplot2)

#Exploring the data 
load("data/tweets_institutions.RData")
```

# Initial exploration of the data 
```{r}



# Average tweets per account 
institutions %>% group_by(screen_name) %>% summarise(n = n()) %>% summarise(avg_tweets = mean(n))

# Less recent tweet by account 
institutions %>% group_by(screen_name) %>% summarize(min = min(created_at)) %>% arrange(desc(min))

# Create a varaible with day of publishing
institutions <- institutions %>% 
  mutate(created_at = as_date(created_at),
         day_published = ymd(created_at)) 

# Frequency of tweets across time: too skewed due to some accounts tweeting too little
frequency_inst_all <- institutions %>% 
  group_by(day_published) %>% 
  summarize(tweets_per_day = n()) %>% 
    ggplot(aes(x = day_published,
               y = tweets_per_day)) +
    geom_line(color = "#F59824")+
    theme_minimal() +
  labs(title = "Frequency of tweets as scraped with Twitter's API", 
       subtitle = "3200 tweets per account (institutional accounts)", 
       x = "Date",
       y = "Tweets per day")

inst_jul_nov_2020 <- institutions %>% filter(day_published >= "2020-07-01")    

# Frequency of tweets per day from July 2020 to November 2020: more balanced
frequency_inst_trimmed <- inst_jul_nov_2020 %>% 
   group_by(day_published) %>% 
  summarize(tweets_per_day = n()) %>% 
    ggplot(aes(x = day_published,
               y = tweets_per_day)) +
    geom_line(color = "#F59824")+
    theme_minimal() +
    labs(title = "Frequency of scraped tweets Jul-Dec 2020",
       x = "Date", 
       y = "Tweets per day")

ggsave(frequency_inst_all, file = "figures/3_frequency_ins_all_time.png", height = 3, width = 5)
ggsave(frequency_inst_trimmed, file = "figures/4_frequency_ins_jul_nov.png", height = 3, width = 5)

# Number of accounts left after trimming data : 39 
unique(inst_jul_nov_2020$name.x) %>% length()

# Number of tweets left after trimming: 25941 
dim(inst_jul_nov_2020)
```

```{r}

inst_jul_nov_2020 <- inst_jul_nov_2020 %>% 
  mutate(id = rownames(inst_jul_nov_2020),
         text_id = paste(screen_name, id,
                         sep = "_"))

save(inst_jul_nov_2020, file = "data/inst_jul_nov_2020.RData")

```

```{r}

inst_jul_nov_2020 <- 
  inst_jul_nov_2020 %>% 
  select(name.x, 
         screen_name, 
         text,
         user_id, 
         created_at, 
         retweet_count,
         text_id)

#Creating corpus
institutions_corp <- corpus(inst_jul_nov_2020)
```

```{r}
# Constructing a document feature matrix with Quanteda 
# Removing unnecessary words; stem the text, extract n-grams, remove punctuation, keep Twitter features…

corpdfm_i <- dfm(institutions_corp, 
             tolower = TRUE, # convert all words to lower case 
             stem = TRUE, # Esto lo hace en inglés. Busca la raíz de la palabra. Va a tocar hacerlo aparte... 
             remove = c(stopwords("spanish")),
             remove_punct = TRUE, # remove punctuation
             remove_url = TRUE, # remove u
             remove_numbers = TRUE,
             remove_symbols = TRUE,
             verbose = TRUE) # Bueno hacer esta opción para ver que está haciendo el dfm.
             
# Remove mentions (@)
corpdfm_i <- dfm_remove(corpdfm_i,
                      "@*",
                      verbose = T)

#Remove hashtags         
corpdfm_i <- dfm_remove(corpdfm_i,
                      "#*",
                      verbose = T)

# Remove more stopwords (less than 4 characters)
corpdfm_i <- dfm_keep(corpdfm_i,
                    min_nchar = 4,
                      verbose = T)

# Remove additional words that do not do meaningful work 
load("data/additional_stopwords.RData")

corpdfm_i <- dfm_remove(corpdfm_i,
                    additional_stopowrds,
                      verbose = T)

docnames(corpdfm_i) <- inst_jul_nov_2020$text_id

institutions_dfm_full <- corpdfm_i

#Save full DFM 
#save(institutions_dfm_full, file = "data/full_institutions_dfm.Rdata")

#DFM sample used temporarily to test out topic models
#sample_institutions_topicmodel <- dfm_sample(x = corpdfm_i, size = 7000, margin = "documents")

# Save the sample dfm to use for topic modelling and scaling 
#save(sample_institutions_topicmodel, file = "data/sample_institutions_topicmodel.RData")

```