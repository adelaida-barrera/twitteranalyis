---
title: "Topic model institutions"
output: html_document
---
```{r}
library(tidyverse)
library(stm)
library(quanteda)
library(tidytext)
library(ggthemes)
```

# Get data and clean it. Create dfm. 
```{r}

# Load data with tweets from institutional twitter accounts between july and november 2020. 

load("data/pruebas/inst_jul_nov_2020.RData")

institutions <- inst_jul_nov_2020

institutions <- 
  institutions %>% 
  select(name.x, 
         screen_name, 
         text,
         user_id, 
         created_at, 
         retweet_count)


#Creating corpus
institutions_corp <- corpus(institutions)

# Removing unnecessary words; stem the text, extract n-grams, remove punctuation, keep Twitter features…
institutions_dfm <- dfm(institutions_corp, 
             tolower = TRUE, # convert all words to lower case 
             remove = c(stopwords("spanish")),
             remove_punct = TRUE, # remove punctuation
             remove_url = TRUE, # remove u
             remove_numbers = TRUE,
             remove_symbols = TRUE,
             verbose = TRUE) # Bueno hacer esta opción para ver que está haciendo el dfm.
            #it is better not to stem for topic models  
             
# Remove mentions (@)
institutions_dfm <- dfm_remove(institutions_dfm,
                      "@*",
                      verbose = T)

#Remove hashtags         
institutions_dfm <- dfm_remove(institutions_dfm,
                      "#*",
                      verbose = T)

# Remove more stopwords (less than 4 characters)
institutions_dfm <- dfm_keep(institutions_dfm,
                    min_nchar = 4,
                      verbose = T)

# Remove additional words that do not do meaningful work 
load("additional_stopwords.RData")

institutions_dfm <- dfm_remove(institutions_dfm,
                    additional_stopowrds,
                      verbose = T)

#Creating a random sample of 7000 tweets to test the topic model (full dfm is too large)
sample_institutions <- dfm_sample(x = institutions_dfm, size = 7000, margin = "documents")
```

#Running the structural topic model

```{r}

#Make the quanteda dfm a stm corpus for running the model
stm_sample_institutions <- asSTMCorpus(sample_institutions)

# Getting info from the corpus
out_i <- prepDocuments(stm_sample_institutions$documents, 
                     stm_sample_institutions$vocab, 
                     stm_sample_institutions$meta)

# Running the structural topic model, defining 10 topics. 
model_topics_institutions <- stm(documents = out_i$documents, 
              vocab = out_i$vocab, 
              K = 10, seed = 12345)



```

### Save it 

```{r}
# Save the output for future plotting or analysis 
save(model_topics_institutions, file = "data/pruebas/model_topics_institutions")

```

#See what came out 

```{r}

plot(model_topics_institutions, type = "labels", labeltype = "prob", topics = (1:3)) # or frex, lift, score
plot(model_topics_institutions, type = "labels", labeltype = "prob", topics = (4:6)) 
plot(model_topics_institutions, type = "labels", labeltype = "prob", topics = (7:9)) 
plot(model_topics_institutions, type = "labels", labeltype = "prob", topics = 10)
 
```
# Plotting prevalence of topics in corpus 

```{r}

# Trying to plot as in https://juliasilge.com/blog/evaluating-stm/
# Hay que entender bien qué significan las betas y las gammas :S 

td_beta <- tidy(model_topics_institutions)

td_gamma <- tidy(model_topics_institutions, matrix = "gamma",
                 document_names = rownames(institutions_dfm))

top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

topics_inst_plot <- gamma_terms %>%
  top_n(20, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0.2, nudge_y = 0.0005, size = 2.5) +
  coord_flip() +
  theme_minimal() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.3)) +
  labs(x = NULL, y = expression(gamma),
       title = "Topics by prevalence in the Colombian feminist institutions' twitter discourse",
       subtitle = "Top words that contribute to each topic")

```


```{r}

ggsave("figures/topics_inst_plot.jpg")

```