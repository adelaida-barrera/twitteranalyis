---
title: "Data cleaning and exploration"
author: "Adelaida Barrera"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(quanteda)
library(readtext) # for getting document and their info into a data frame 
library(lubridate)
```

```{r}

# Load dataframe with all tweets from individual accounts scraped from twitter
load("data/tweets_feminists.RData")

#Create a new variable with day of publication
feminists <- feminists %>% 
  mutate(created_at = as_date(created_at),
         day_published = ymd(created_at)) 
```

# Initial exploration of the data of individual feminist accounts 

```{r}

# Average tweets per account 
feminists %>% group_by(screen_name) %>% summarise(n = n()) %>% summarise(avg_tweets = mean(n))

# Less recent tweet by account 
feminists %>% group_by(screen_name) %>% summarize(min = min(created_at)) %>% arrange(desc(min))

# Frequency of tweets across time: too skewed due to some accounts tweeting too little
 
frequency_ind_all <- feminists %>% 
  group_by(day_published) %>% 
  summarize(tweets_per_day = n()) %>% 
    ggplot(aes(x = day_published,
               y = tweets_per_day)) +
    geom_line(color = "#3EA3B3")+
    theme_minimal()+
   labs(title = "Frequency of tweets as scraped with Twitter API",
        subtitle = "3200 tweets per account (individual accounts)",
        x = "Date", 
        y = "Tweets per day")
       

# Trim dataframe to leave only last 6 months

fem_jul_nov_2020 <- feminists %>% filter(day_published >= "2020-07-01")       

# Frequency of tweets per day from July 2020 to November 2020: more balanced

frequency_ind_trimmed <- fem_jul_nov_2020 %>% 
  group_by(day_published) %>% 
  summarize(tweets_per_day = n()) %>% 
    ggplot(aes(x = day_published,
               y = tweets_per_day)) +
    geom_line(color = "#3EA3B3")+
    theme_minimal() +
  labs(title = "Frequency of scraped tweets in Jul-Dec 2020",
       x = "Date", 
       y = "Tweets per day")

# Number of accounts left after trimming data : 67 
unique(fem_jul_nov_2020$name) %>% length()

# Number of tweets left after trimming: 116402 
dim(fem_jul_nov_2020)

ggsave(frequency_ind_all, file = "figures/1_frequency_ind_all_time.png")
ggsave(frequency_ind_trimmed, file = "figures/2_frequency_ind_jul_nov.png")

```

# Trimming dataframe 
```{r}

# Removing the congressswomen to get more stable speech (decided after exploring with topic models)

non_congress <- fem_jul_nov_2020 %>%
  filter(main_occupation != "congressperson") 

# ! Important, create a variable identifying the document with name of user and number of tweet 
non_congress <- non_congress %>% 
  mutate(id = rownames(non_congress),
         text_id = paste(screen_name, id,
                         sep = "_"))

save(non_congress, file = "data/non_congress_fem_jul_nov_2020.RData")
```

#Clean data and create sample for topic model of feminist accounts

```{r}

# From the 90+ variables in the tweetr output, select the relevant one for the analysis

non_congress <- 
  non_congress %>% 
  select(name, 
         screen_name, 
         text,
         user_id, 
         text_id,
         created_at, 
         retweet_count,
         main_occupation,
         institution)

#Creating corpus
non_congress_corp <- corpus(x = non_congress)

# Removing unnecessary words; stem the text, extract n-grams, remove punctuation, keep Twitter features…
non_congress_dfm <- dfm(x = non_congress_corp, 
                        tolower = TRUE, # convert all words to lower case 
                        remove = c(stopwords("spanish")),
                        remove_punct = TRUE, # remove punctuation
                        remove_url = TRUE, # remove u
                        remove_numbers = TRUE,
                        remove_symbols = TRUE,
                        verbose = TRUE) # Bueno hacer esta opción para ver que está haciendo el dfm.
                #it is better not to stem for topic models  
             
# Remove mentions (@)
non_congress_dfm <- dfm_remove(non_congress_dfm,
                      "@*",
                      verbose = T)

#Remove hashtags         
non_congress_dfm <- dfm_remove(non_congress_dfm,
                      "#*",
                      verbose = T)

# Remove more stopwords (less than 4 characters)
non_congress_dfm <- dfm_keep(non_congress_dfm,
                    min_nchar = 4,
                      verbose = T)

# Remove additional words that do not do meaningful work 
load("data/additional_stopwords.RData")

non_congress_dfm <- dfm_remove(non_congress_dfm,
                    additional_stopowrds,
                      verbose = T)

# Important! Change rownames of dfm matrix to the id of each document that includes name of author
docnames(non_congress_dfm) <- non_congress$text_id

non_congress_dfm_full <- non_congress_dfm

save(non_congress_dfm_full, file = "data/non_congress_dfm_full")

```


```{r}

# For testing the model, create a subset to avoid long processing times. This was used to define number of topics. Later the model was run in the whole dfm.

#Creating a random sample of 7000 tweets to test the topic model (full dfm is too large)
#sample_non_congress_topicmodel <- dfm_sample(x = non_congress_dfm, size = 7000, margin = "documents")

#save(sample_non_congress_topicmodel, file = "data/sample_non_congress_topicmodel.Rdata")
```


