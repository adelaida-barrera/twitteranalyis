---
title: "Data cleaning and exploration"
author: "Adelaida Barrera"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(quanteda)
library(readtext) # for getting document and their info into a data frame 
library(tm)
library(lubridate)

# Set seed


#Exploring the data 

load("data/tweets_feminists.RData")
```

# Initial exploration of the data of individual feminist accounts 

```{r}

# Average tweets per account 
feminists %>% group_by(screen_name) %>% summarise(n = n()) %>% summarise(avg_tweets = mean(n))

# Less recent tweet by account 
feminists %>% group_by(screen_name) %>% summarize(min = min(created_at)) %>% arrange(desc(min))

# Frequency of tweets across time: too skewed due to some accounts tweeting too little
 
feminists <- feminists %>% 
  mutate(created_at = as_date(created_at),
         day_published = ymd(created_at)) 

feminists %>% 
  group_by(day_published) %>% 
  summarize(tweets_per_day = n()) %>% 
    ggplot(aes(x = day_published,
               y = tweets_per_day)) +
    geom_line()+
    theme_minimal()

fem_jul_nov_2020 <- feminists %>% filter(day_published >= "2020-07-01")       

# Frequency of tweets per day from July 2020 to November 2020: more balanced
fem_jul_nov_2020 %>% 
  group_by(day_published) %>% 
  summarize(tweets_per_day = n()) %>% 
    ggplot(aes(x = day_published,
               y = tweets_per_day)) +
    geom_line()+
    theme_minimal()

# Number of accounts left after trimming data : 67 
unique(fem_jul_nov_2020$name) %>% length()

# Number of tweets left after trimming: 116402 
dim(fem_jul_nov_2020)

# Removing the congressswomen to get more stable speech
non_congress_fem_jul_nov_2020 <- fem_jul_nov_2020 %>% filter(main_occupation != "congressperson")
save(non_congress_fem_jul_nov_2020, file = "data/non_congress_fem_jul_nov_2020.RData")
```

#Clean data and create sample for topic model of feminist accounts

```{r}

non_congress <- non_congress_fem_jul_nov_2020

non_congress <- 
  non_congress %>% 
  select(name, 
         screen_name, 
         text,
         user_id, 
         created_at, 
         retweet_count,
         main_occupation,
         institution)

#Creating corpus
non_congress_corp <- corpus(non_congress)

# Removing unnecessary words; stem the text, extract n-grams, remove punctuation, keep Twitter features…
non_congress_dfm <- dfm(non_congress_corp, 
             tolower = TRUE, # convert all words to lower case 
             remove = c(stopwords("spanish")),
             remove_punct = TRUE, # remove punctuation
             remove_url = TRUE, # remove u
             remove_numbers = TRUE,
             remove_symbols = TRUE,
             verbose = TRUE) # Bueno hacer esta opción para ver que está haciendo el dfm.
            #it is better not to stem for topic models  
             
# Remove mentions (@)
non_congress_dfm <- dfm_remove(non_congress_dfm,
                      "@*",
                      verbose = T)

#Remove hashtags         
non_congress_dfm <- dfm_remove(non_congress_dfm,
                      "#*",
                      verbose = T)

# Remove more stopwords (less than 4 characters)
non_congress_dfm <- dfm_keep(non_congress_dfm,
                    min_nchar = 4,
                      verbose = T)

# Remove additional words that do not do meaningful work 
load("data/additional_stopwords.RData")

non_congress_dfm <- dfm_remove(non_congress_dfm,
                    additional_stopowrds,
                      verbose = T)

#Creating a random sample of 7000 tweets to test the topic model (full dfm is too large)
sample_non_congress_topicmodel <- dfm_sample(x = non_congress_dfm, size = 7000, margin = "documents")

save(sample_non_congress_topicmodel, file = "data/sample_non_congress_topicmodel.Rdata")
```


