---
title: "Data cleaning and exploration"
author: "Adelaida Barrera"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(quanteda)
library(readtext) # for getting document and their info into a data frame 
library(tm)
library(lubridate)
#Exploring the data 

load("data/tweets_feminists.RData")

# Average tweets per account 
feminists %>% group_by(screen_name) %>% summarise(n = n()) %>% summarise(avg_tweets = mean(n))

# Less recent tweet by account 
feminists %>% group_by(screen_name) %>% summarize(min = min(created_at)) %>% arrange(desc(min))


                                  


```

```{r}

names(feminists)
#Subsetting dataset

feminists <- 
  feminists %>% 
  select(name, 
         screen_name, 
         text,
         user_id, 
         created_at, 
         retweet_count,
         main_occupation,
         institution)

#Creating corpus
feminists_corp <- corpus(feminists)
```


```{r}
## Corpus structure

#A corpus has two important components, the texts themselves, and information 
#about them called 'docvars'. 

#Explore docvars
head(docvars(feminists_corp))

#Explore text
#txts <- texts(feminists_corp)

#The information in summary is available in different functions

#ndoc(feminists_corp) # document count
#docnames(feminists_corp) # unique document identifiers
#ntype(feminists_corp) # types in each document
#ntoken(feminists_corp) # tokens in each document
#nsentence(feminists_corp) # sentences in each document

```
```{r}
#Subset on the basis of the docvars
#sample_accounts <- c("1354646353")

#Those word and sentence counts come from the `ntype`, (vocabulary size)
#`ntoken` (word count), and `nsentence` (sentence count)
#functions.

#Getting tokens

#toks <- tokens(feminists_corp)

#Subsetting the tokens

#toks [[10]]
#toks [1:3]

```

```{r}
#Removing stopwords (leaving gaps were they are)

#toks2 <- tokens_remove(toks, stopwords(), padding = TRUE)

#toks2[[1]][1:20] # first 20 tokens of document 1

```

```{r}
#Keywords in context
#kw_feminism <- kwic(feminists_corp, "feminismo", window = 10)
#head(kw_feminism)
#view(kw_feminism)

#Paste pre + post

#feminism_df <- tibble(speaker = kw_feminism$docname, 
              #    text = paste(kw_feminism$pre, kw_feminism$post, sep = " "))

#corp_feminism <- corpus(feminism_df)
#summary(corp_feminism)

```


```{r}
# Constructing a document feature matrix with Quanteda 
corpdfm <- dfm(feminists_corp)

dim(corpdfm)

featnames(corpdfm)[1:1000]
docnames(corpdfm)

# Removing unnecessary words; stem the text, extract n-grams, remove punctuation, keep Twitter features…
corpdfm <- dfm(feminists_corp, 
             tolower = TRUE, # convert all words to lower case 
             stem = TRUE, # Esto lo hace en inglés. Busca la raíz de la palabra. Va a tocar hacerlo aparte... 
             remove = c(stopwords("spanish")),
             remove_punct = TRUE, # remove punctuation
             remove_url = TRUE, # remove u
             remove_numbers = TRUE,
             remove_symbols = TRUE,
             verbose = TRUE) # Bueno hacer esta opción para ver que está haciendo el dfm.
             
# Remove mentions (@)
corpdfm <- dfm_remove(corpdfm,
                      "@*",
                      verbose = T)

#Remove hashtags         
corpdfm <- dfm_remove(corpdfm,
                      "#*",
                      verbose = T)

# Remove more stopwords (less than 4 characters)
corpdfm <- dfm_keep(corpdfm,
                    min_nchar = 4,
                      verbose = T)

sample_quanteda <- dfm_sample(x = corpdfm, size = 23000, margin = "documents")

# Save the sample dfm to use in tests for topic modelling and scaling 
save(sample_quanteda, file = "data/Sample_quanteda.RData")

```

```{r}
##### Otros métodos para limpiar y procesar el dfm
##### Otros métodos #####

mycorpus <- Corpus(VectorSource(feminists$text))
mycorpus <- tm_map(mycorpus, removeWords, stopwords("spanish"))

# Remove URL
remove_url <- function(x) gsub("http[^[:space:]]*","",x)
mycorpus <- tm_map(mycorpus, content_transformer(remove_url))

# Remove anything other than spanish letters and space
remove_other <- function(x) gsub("[^[:alpha:][:space:]]*","", x) 
mycorpus <- tm_map(mycorpus, content_transformer(remove_other))
mycorpus <- tm_map(mycorpus, content_transformer(removePunctuation))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, stemDocument)

mycorpus <- corpus(mycorpus)
mycorpus_dfm <- dfm(mycorpus)

#Create a sample to try analysis 
sample_tm <- dfm_sample(x = mycorpus_dfm, size = 1000, margin = "documents")


```

```{r}
# Comparing both dfm's (quanteda vs tm)
dim(mycorpus_dfm)
dim(corpdfm)

featnames(mycorpus_dfm)[1:1000]
featnames(corpdfm)[1:1000]


```

